{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3 Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8XqvGqo20DC",
        "colab_type": "text"
      },
      "source": [
        "Importing required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV-4T4PH26Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "# import pybullet_envs\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbxwrrs4Pq0",
        "colab_type": "text"
      },
      "source": [
        "Experience replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTtiI5xc4EWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, max_size = 1e6):\n",
        "        self.max_size = max_size\n",
        "        self.storage = []\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[self.ptr] = transition\n",
        "            self.ptr = (self.position + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(None)\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        index = np.random.randint(0,len(self.storage),batch_size)\n",
        "        batch_states, batch_next_states, batch_actions, batch_rewards, \\\n",
        "        batch_done = [],[],[],[],[]\n",
        "        for i in ind :\n",
        "            state,next_state,action,reward,done = self.storage[i]\n",
        "            batch_states.append(np.array(state, copy = False))\n",
        "            batch_next_states.append(np.array(next_state, copy = False))\n",
        "            batch_actions.append(np.array(action, copy = False))\n",
        "            batch_rewards.append(np.array(reward, copy = False))\n",
        "            batch_done.append(np.array(done, copy = False))\n",
        "        return np.array(batch_states), np.array(batch_next_states), \\\n",
        "              np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), \\\n",
        "              np.array(batch_done).reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXCKnihB9l-J",
        "colab_type": "text"
      },
      "source": [
        "Neural Network for   **ACTOR model**   and    **ACTOR Target**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl3bmEKb9iu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self,state_dims, action_dims, max_action):\n",
        "    # activate Inheritance , Intialize all the variables of Parent class\n",
        "    super(Actor,self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dims, 400)\n",
        "    self.layer_2 = nn.Linear(400,300)\n",
        "    self.layer_3 = nn.Linear(300,action_dims)\n",
        "    #max_action is to clip in case we added too much noise\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKAXJGI-Dicg",
        "colab_type": "text"
      },
      "source": [
        "DNN for two  **Critic model** and **Critic Target**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U7AbdS1Df5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self,state_dims, action_dims):\n",
        "    # activate Inheritance , Intialize all the variables of Parent class\n",
        "    super(Actor,self).__init__()\n",
        "    # First Critic Network\n",
        "    self.layer_1 = nn.Linear(state_dims + action_dims, 400)\n",
        "    self.layer_2 = nn.Linear(400,300)\n",
        "    self.layer_3 = nn.Linear(300,action_dims)\n",
        "\n",
        "    # Second Critic Network\n",
        "    self.layer_4 = nn.Linear(state_dims + action_dims, 400)\n",
        "    self.layer_5 = nn.Linear(400,300)\n",
        "    self.layer_6 = nn.Linear(300,action_dims)\n",
        "\n",
        "\n",
        "  def forward(self, x, u): # x- state , u = action\n",
        "    xu = torch.cat([x,u], 1) # Conacatenation of states and actions as input\n",
        "    # forward propagation on First Critic \n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "\n",
        "    # forward propagation on Second Critic \n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "\n",
        "    return x1,x2\n",
        "\n",
        "\n",
        "  def Q1(self, x, u): # For updating Q values\n",
        "    xu = torch.cat([x,u],1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HZFbm21G9F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReAlvqVYGOgY",
        "colab_type": "text"
      },
      "source": [
        "**T3D**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1tCvyVoGUAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class T3D(object):\n",
        "\n",
        "    def __init__(self,state_dims, action_dims, max_action):\n",
        "\n",
        "      self.actor = Actor(state_dims, action_dims, max_action).to(device)\n",
        "      self.actor_target = Actor(state_dims, action_dims, max_action).to(device)\n",
        "      # Intializing with model weights to keeo them same\n",
        "      self.actor_target.load_state_dict(self.actor.state_dict)\n",
        "      self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "      self.critic = Critic(state_dims, action_dims).to(device)\n",
        "      self.critic_target = Critic(state_dims, action_dims).to(device)\n",
        "      # Intializing with model weights to keeo them same\n",
        "      self.critic_target.load_state_dict(self.critic.state_dict)\n",
        "      self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "\n",
        "      self.max_action = max_action\n",
        "\n",
        "    def select_action(self,state):\n",
        "      state = torch.Tensor(state.reshape(1.-1)).to(device)\n",
        "      return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100,\n",
        "              discount=0.99, tau = 0.005, policy_noise_clip = 0.5, \n",
        "              policy_freq = 2):\n",
        "      for it in range(iterations):\n",
        "        # Step 4 We sample from a batch of transitions (s,s',a,r) from memory\n",
        "        batch_states, batch_next_states, batch_actions, batch_rewards, \\\n",
        "        batch_done = replay_buffer.sample(batch_size) \n",
        "        state = torch.Tensor(batch_states).to(device)\n",
        "        next_state = torch.Tensor(batch_next_states).to(device)\n",
        "        action = torch.Tensor(batch_actions).to(device)\n",
        "        reward = torch.Tensor(batch_reward).to(device)\n",
        "        done = torch.Tensor(batch_done).to(device)\n",
        "  \n",
        "        # Step 5 : From the next state s',the actor target plays the next actions a'\n",
        "        next_action = self.actor_target.forward(next_state)\n",
        "\n",
        "        # Step-6 We add Gaussian noise to this next action a' and and we clamp\n",
        "        # it in a range of values supported by this environment\n",
        "        noise = torch.Tensor(batch_actions).data.normal_(0,policy_noise).to(device)\n",
        "        noise = noise.clamp(-noise_clip, noise_clip)\n",
        "        next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "        # Step-7  The two critic models takes input (s' , a') and return two Q values, \n",
        "        # Qt1(s',a') and Qt2(s' ,a') as outputs)\n",
        "        targetQ1, targetQ2 = self.critic_target.forward (next_state, next_action)\n",
        "\n",
        "        #Step-8 : Take the minimum of these two Q-values\n",
        "        target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "        # Step-9 : We get final target of the two critic model,which is:\n",
        "        #Qt = r + gamma*min(Qt1,Qt2)\n",
        "        target_Q = reward + ((1-done) * discount * target_Q).detach()\n",
        "\n",
        "        # Step 10 :The two critic models take each the couple(s,a)\n",
        "        # as input and return two  Q values\n",
        "        current_Q1, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "        # Step-11 : We compute the loss coming from the two Critic models\n",
        "        critic_loss = F.mse_loss(current_Q1,target_Q) + F.mse_loss(current_Q2,target_Q)\n",
        "\n",
        "        #Step-12 We backpropagate this critic Loss and update the parameters\n",
        "        #of the two critic models with a Adam optimizer\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "\n",
        "        #Step 13: Once every two iterations , we update our Actor model by\n",
        "        # performing gradient ascent on the output off the first critic model\n",
        "        if it % policy_freq == 0:\n",
        "          #DPG\n",
        "          actor_loss = -(self.critic.Q1(state, self.actor(state)).mean())\n",
        "          self.actor_optimizer.grad_zero()\n",
        "          actor_loss.backward()\n",
        "          self.actor_optimizer.step()\n",
        "\n",
        "          #Step-14: Still once every two iterations, we update the weights of the\n",
        "          # of the Actor target by Polyak averaging\n",
        "          for param, target_param in zip(self.actor.parameters(),self.actor_target.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "          #Step-15 : Still once every two iterations, we update the weights of the \n",
        "          #Critic target by Polyak averaging\n",
        "          for param, target_param in zip(self.critic.parameters(),self.critic_target.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rql6HOOHKff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}